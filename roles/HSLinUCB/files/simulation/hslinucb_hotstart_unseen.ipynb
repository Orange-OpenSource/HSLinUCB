{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software Name : HSLinUCB\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2021 Orange\n",
    "# SPDX-License-Identifier: GPL-2.0\n",
    "#\n",
    "# This software is distributed under the GNU General Public License v2.0 license\n",
    "#\n",
    "# Author: David DELANDE <david.delande@orange.com> et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This jupyter notebook requires a HSLinUCB model. Executes hslinucb_coldstart.ipynb to generate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from math import *\n",
    "import scipy.stats as stats\n",
    "from numpy.linalg import inv\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy.linalg\n",
    "import copy\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm import trange\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.random.seed(1)\n",
    "\n",
    "import pickle\n",
    "from collections import deque , OrderedDict\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import h5py\n",
    "from lib.CogscalingLib import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionFile=\"D2.h5\" #Set the dataset to use. Other possible dataset: D2.h5. For ColdStart and hotstart seen uses D1.h5. For hotstart unseen uses D2.h5\n",
    "environment_orchestrator = Orchestrator(debug=False, sessionFile=sessionFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvTest():\n",
    "\n",
    "    \n",
    "    def __init__(self, narms, level=1,max_level = 25, latency_ref = 600, debug = False):\n",
    "        self.latency_ref = latency_ref\n",
    "        self.min_level = 1\n",
    "        self.max_level = max_level\n",
    "        self.level = level\n",
    "        self.debug = debug\n",
    "        self.narms = narms\n",
    "        self.loads = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "        self.levels = [1,2,3,4,5,6,7,8,9,10]\n",
    "        self.sessionFile=sessionFile\n",
    "        self.oracle_cheat = np.full((np.max(self.loads) +1, len(self.levels) + 1),2)\n",
    "        self.previous_states = []\n",
    "        self.current_states = []\n",
    "        self.previous_duration = 0\n",
    "        self.current_duration = 0\n",
    "        self.previous_rq = 0\n",
    "        self.current_rq = 0\n",
    "        self.targetPattern = []\n",
    "        self.rounds = 0\n",
    "        self.replay = True\n",
    "        self.record = False\n",
    "        super().__init__()\n",
    "        return\n",
    "    \n",
    "    def reset(self, narms, target, level,max_level, latency_ref):\n",
    "        self.level = level\n",
    "        self.rounds = 0\n",
    "        self.step = 0\n",
    "    \n",
    "    def moving_mean(self,measure,order):\n",
    "        result = []\n",
    "        if order%2 == 0:\n",
    "            m = order / 2\n",
    "            m = int(m)\n",
    "            for index in range(0, len(measure)):\n",
    "                if index < m or index + m + 1 > len(measure):\n",
    "                    continue\n",
    "                sum1 = 0\n",
    "                sum2 = 0\n",
    "                for index_value in range(index -m, index + m):\n",
    "                    sum1 = sum1 + measure[index_value]\n",
    "                    sum2 = sum2 + measure[index_value + 1]\n",
    "                mean1 = (1/(2*m)) * sum1\n",
    "                mean2 = (1/(2*m)) * sum2\n",
    "                result.append((mean1 + mean2) / 2)\n",
    "        else:\n",
    "            m = (order - 1) / 2\n",
    "            m = int(m)\n",
    "            for index in range(0, len(measure)):\n",
    "                if index < m or index + m + 1 > len(measure):\n",
    "                    continue   \n",
    "                sum1 = 0\n",
    "                for index_value in range(index - m, index + m + 1):\n",
    "                    sum1 = sum1 + measure[index_value]\n",
    "                result.append((1/((2*m)+1)) * sum1)\n",
    "        return result\n",
    "    \n",
    "    def save_PatternModel(self,file = ''):\n",
    "        #Save the target pattern to a json file\n",
    "        if file == '':\n",
    "            file = 'pattern'\n",
    "        with open(file + '.json', 'w') as filehandle:\n",
    "            json.dump(self.targetPattern, filehandle)\n",
    "\n",
    "    def load_PatternModel(self, file = ''):\n",
    "        #Load the target pattern from a json file\n",
    "        if file == '':\n",
    "            file = 'pattern'\n",
    "        with open(file + '.json') as json_file:\n",
    "            self.targetPattern = json.load(json_file)\n",
    "        return self.targetPattern\n",
    "    \n",
    "    def display_PatternModel(self):\n",
    "        #Generate the target pattern graph\n",
    "        f = plt.figure()\n",
    "        patternGraph = f.add_subplot(111)\n",
    "        patternGraph.plot(self.targetPattern)\n",
    "        plt.ylabel(\"Number of users\",fontsize=16)\n",
    "        plt.xlabel(\"Steps\",fontsize=16)\n",
    "        plt.title(\"Workload injection pattern\");\n",
    "        plt.grid();\n",
    "        plt.show()\n",
    "    \n",
    "    def generateRealProgressiveTargetPattern(self,nrounds,step,target_list):\n",
    "        #Generate the target pattern\n",
    "        self.targetPattern = []\n",
    "        patternChangeNumber = math.floor(nrounds/step)\n",
    "\n",
    "        target = target_list[0]\n",
    "        target_list_index = 0\n",
    "        mode = 1 #1 for increase, 0 for decrease\n",
    "        p_c = 1\n",
    "        for n in range(nrounds): \n",
    "                if n < (p_c * step):\n",
    "                    self.targetPattern.append(target)\n",
    "                elif p_c <= patternChangeNumber:\n",
    "                    p_c = p_c + 1\n",
    "                    if mode == 1:\n",
    "                        if target_list_index < (len(target_list) - 1):\n",
    "                            target_list_index += 1\n",
    "                        else:\n",
    "                            mode = 0\n",
    "                    else:\n",
    "                        if target_list_index > 0:\n",
    "                            target_list_index -= 1\n",
    "                        else:\n",
    "                            mode = 1\n",
    "                    target = target_list[target_list_index]\n",
    "                    self.targetPattern.append(target)\n",
    "                else:\n",
    "                    self.targetPattern.append(target)\n",
    "        return self.targetPattern\n",
    "    \n",
    "    def changeInjector(self):\n",
    "        user_level = self.targetPattern[self.rounds]\n",
    "        if self.debug:\n",
    "            print(\"injector level:\", user_level)\n",
    "        if self.replay == False:\n",
    "            status, message = environment_orchestrator.setLocustUser(user=int(user_level),spawn_rate=1)\n",
    "            print(\"message:\", message)\n",
    "                                    \n",
    "    def getContext(self):\n",
    "        states = []            \n",
    "        if len(self.previous_states) == 0:\n",
    "            state = environment_orchestrator.getAgregatedState(components=[{'prometheus': 'front-dynamic-component','zipkin': 'front-dynamic-component-service.default.svc.cluster.local:80/*'}],replay=self.replay, record=self.record,load=self.targetPattern[self.rounds], level=self.level,useMetricServer = False)\n",
    "            self.previous_states.append(state['lastcomponentNumber'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['duration'].to_numpy().astype(float)[0])\n",
    "            self.previous_duration = state['duration'].to_numpy().astype(float)[0]\n",
    "            self.previous_states.append(state['req_perc_sec'].to_numpy().astype(float)[0])\n",
    "            self.previous_rq = state['req_perc_sec'].to_numpy().astype(float)[0]\n",
    "            self.previous_states.append(state['cpu_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['cpu_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['mem_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['mem_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        elif len(self.previous_states) != 0 and len(self.current_states) != 0:\n",
    "            self.previous_states = self.current_states\n",
    "            self.previous_duration = self.current_duration\n",
    "            self.previous_rq = self.current_rq\n",
    "\n",
    "        self.current_states = []\n",
    "        state = environment_orchestrator.getAgregatedState(components=[{'prometheus': 'front-dynamic-component','zipkin': 'front-dynamic-component-service.default.svc.cluster.local:80/*'}],replay=self.replay, record=self.record,load=self.targetPattern[self.rounds], level=self.level,useMetricServer = False)\n",
    "\n",
    "\n",
    "        response_times = state['response_times'][0] \n",
    "        num_reqs_perc_sec = state['num_reqs_per_sec'][0]\n",
    "        num_fail_per_sec = state['num_fail_per_sec'][0]\n",
    "        self.current_states.append(state['lastcomponentNumber'].to_numpy().astype(float)[0])  \n",
    "        self.current_states.append(state['duration'].to_numpy().astype(float)[0])\n",
    "        self.current_duration = state['duration'].to_numpy().astype(float)[0]\n",
    "        self.current_states.append(state['req_perc_sec'].to_numpy().astype(float)[0])\n",
    "        self.current_rq = state['req_perc_sec'].to_numpy().astype(float)[0]\n",
    "        self.current_states.append(state['cpu_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['cpu_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['mem_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['mem_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        if self.debug:\n",
    "            print(\"previous states in getcontext:\", self.previous_states)\n",
    "            print(\"current states in getcontext:\", self.current_states)\n",
    "        global_states = np.concatenate((self.previous_states,self.current_states), axis=None)\n",
    "        states = self.current_states\n",
    "        states = np.asmatrix(states)\n",
    "        if self.replay == False:\n",
    "            self.level = int(self.current_states[0])\n",
    "        if self.debug:\n",
    "            print(\"state returned:\", states)\n",
    "        return states\n",
    "    \n",
    "    def armStay(self,context):\n",
    "        if self.debug:\n",
    "            print(\"In action stay function\")\n",
    "            print(\"level in armStay:\", self.level)\n",
    "        previous_context = context\n",
    "        reward = self.computeReward(previous_context=previous_context,context = context, action = \"stay\",actionStep = 0)\n",
    "        return reward, context\n",
    "        \n",
    "    def armUp(self,context,actionStep = 1):\n",
    "        previous_context = context\n",
    "        if self.debug:\n",
    "            print(\"In action up function\")\n",
    "            print(\"actionStep:\", actionStep)\n",
    "            print(\"level in armUp:\", self.level)\n",
    "        if self.level + actionStep > self.max_level:\n",
    "            if self.debug:\n",
    "                print(\"upper limit reached\")\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.replay:\n",
    "                self.level += actionStep\n",
    "            else:\n",
    "                status, message = environment_orchestrator.incrementalKubernetesDeploymentScale(deployment=\"front-dynamic-component\",step=actionStep,waitKubernetes=True,waitPrometheus=True,useMetricServer = False)\n",
    "                print(\"message:\", message)\n",
    "            context = self.getContext()\n",
    "            reward = self.computeReward(previous_context = previous_context,context = context, action = \"up\",actionStep = actionStep)\n",
    "        return reward,context\n",
    "    \n",
    "    def armDown(self,context, actionStep = 1):\n",
    "        previous_context = context\n",
    "        if self.debug:\n",
    "            print(\"In action down function\")\n",
    "            print(\"actionStep:\", actionStep)\n",
    "            print(\"level in armDown:\", self.level)\n",
    "        if self.level + actionStep < self.min_level:\n",
    "            if self.debug:\n",
    "                print(\"lower limit reached\")\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.debug:\n",
    "                print(\"lower limit not reached\")\n",
    "            if self.replay:\n",
    "                self.level += actionStep\n",
    "            else:\n",
    "                status, message = environment_orchestrator.incrementalKubernetesDeploymentScale(deployment=\"front-dynamic-component\",step=actionStep,waitKubernetes=True,waitPrometheus=True,useMetricServer = False)\n",
    "                print(\"message:\", message)\n",
    "            context = self.getContext()\n",
    "            reward = self.computeReward(previous_context = previous_context,context = context, action = \"down\",actionStep = actionStep)\n",
    "        return reward,context\n",
    "    \n",
    "    def computeReward(self,previous_context,context, action,actionStep = 1):\n",
    "        if self.debug:\n",
    "            print(\"in compute reward function\")\n",
    "        if action == \"up\":    \n",
    "\n",
    "            if self.debug:\n",
    "                print(\"compute reward for action up\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.previous_duration > self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        if action == \"down\":\n",
    "            if self.debug:\n",
    "                print(\"compute reward for action down\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.current_duration <= self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "                    \n",
    "        if action == \"stay\":\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"Compute reward for action stay\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.current_duration <= self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "        if self.debug:\n",
    "            print(\"returned reward:\", reward)\n",
    "        return reward\n",
    "        \n",
    "    def computeOracle(self):\n",
    "        with h5py.File(self.sessionFile, \"r\") as f:\n",
    "            for load in self.loads:\n",
    "                bestset = False\n",
    "                for level in self.levels:\n",
    "                    load_grp = f.get(str(load))\n",
    "                    component_grp = load_grp.get('front-dynamic-component')\n",
    "                    level_grp = component_grp.get(str(level))\n",
    "                    d = level_grp.get('measure')\n",
    "                    data = np.asarray(d)\n",
    "                    duration = data[:,3].astype(float)\n",
    "                     \n",
    "                    if np.mean(duration) <= self.latency_ref and np.max(duration) <= self.latency_ref:\n",
    "                        if not bestset:\n",
    "                            self.oracle_cheat[load,level] = 1\n",
    "                            bestset = True\n",
    "                        else:\n",
    "                            self.oracle_cheat[load,level] = 0\n",
    "            if self.debug:\n",
    "                print(\"oracle cheat:\", self.oracle_cheat)\n",
    "\n",
    "    def oracle(self):\n",
    "        #Return the Oracle best action\n",
    "        return self.oracle_cheat[int(self.targetPattern[self.rounds]),int(self.current_states[0])]\n",
    "    \n",
    "    def oracleOptimalLevel(self):\n",
    "        #Return the Oracle best action\n",
    "        return np.where(self.oracle_cheat[int(self.targetPattern[self.rounds])] == 1)[0][0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(Environment, Agent, nrounds=None, tie_break_mode = \"random\",DisplayCumulativeRewardGraph = False, debug = False):\n",
    "\n",
    "    environment_level = []\n",
    "    optimal_environment_level = []\n",
    "    latency_history = []\n",
    "    latency_ref_history = []\n",
    "    injector_level = []\n",
    "\n",
    "    T = 0\n",
    "\n",
    "    for i in range(nrounds):\n",
    "\n",
    "        Environment.rounds = i\n",
    "        Environment.changeInjector()\n",
    "        \n",
    "        \n",
    "        context = Environment.getContext()\n",
    "        optimal_environment_level.append(Environment.oracleOptimalLevel())\n",
    "        if Environment.current_duration > Environment.latency_ref:\n",
    "            reward,next_context = Environment.armUp(context = context,actionStep = 1)\n",
    "        else:\n",
    "            action, estimated_reward, confidence = Agent.select(context,tie_break_mode)\n",
    "            if action == 0 :\n",
    "                reward,next_context = Environment.armDown(context = context,actionStep = -1)\n",
    "\n",
    "            if action == 1: \n",
    "                reward,next_context = Environment.armStay(context = context)\n",
    "            Agent.observe(action, context,next_context, reward, update = True)\n",
    "        \n",
    "        latency_history.append(Environment.current_duration)\n",
    "        injector_level.append(Environment.targetPattern[i])\n",
    "        latency_ref_history.append(Environment.latency_ref)\n",
    "        environment_level.append(Environment.current_states[0])\n",
    "\n",
    "        T +=1\n",
    "        if (DisplayCumulativeRewardGraph and T %1000 == 0):\n",
    "            %matplotlib inline\n",
    "            clear_output(True)\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Number of users\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(injector_level)\n",
    "            ax.grid()\n",
    "            ax.set_title('Simulation hotstart unseen contexts injection pattern')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Number of containers\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(environment_level,label='hslinucb')\n",
    "            ax.plot(optimal_environment_level,label='Oracle')\n",
    "            ax.grid()\n",
    "            ax.set_title('simulation hotstart unseen contexts environment level')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc = 'upper right',prop={'size':20})\n",
    "            plt.show()\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Latency(ms)\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(latency_history,label='hslinucb')\n",
    "            ax.plot(latency_ref_history,label=r'$l^{*}$')\n",
    "            ax.grid()\n",
    "            ax.set_title('simulation hotstart unseen contexts latency')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc = 'upper right',prop={'size':20})\n",
    "            plt.show()\n",
    "\n",
    "    return injector_level,latency_ref_history,latency_history, environment_level, optimal_environment_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate progressive pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrounds = 9000 #2400 #Number of step pattern to generate. It must be the same as the one that will be used in simulation\n",
    "randomTargetStep = 50 #The pattern will change every randomTargetStep\n",
    "\n",
    "target_list = [5,10,15,20,25,30,35,40,45,50]\n",
    "#Generate the environment\n",
    "Environment = EnvTest(narms=3,level=1,max_level = 4)\n",
    "#Generate the pattern\n",
    "pattern = Environment.generateRealProgressiveTargetPattern(nrounds = nrounds,step = randomTargetStep, target_list = target_list)\n",
    "Environment.save_PatternModel()\n",
    "#Load pattern from a json file(pattern.json). This method has to be called each time an environment needs to use the target pattern\n",
    "Environment.load_PatternModel()\n",
    "#Display graphically the target pattern model\n",
    "Environment.display_PatternModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB():\n",
    "    \n",
    "    def __init__(self, nArms, nFeature, alpha = 1, inversion_interval = 5, _lambda = 1):\n",
    "        self._nArms = nArms\n",
    "        self._nFeature = nFeature\n",
    "        self._alpha = alpha\n",
    "        self._lambda = _lambda\n",
    "        self._inversion_interval = inversion_interval\n",
    "        self.reset()\n",
    "        \n",
    "    def export_model(self):\n",
    "        return self._A, self._theta\n",
    "    \n",
    "    def load_model(self):\n",
    "        with open(\"save_agent_linucb_A_coldstart.bin\", 'rb') as f:\n",
    "            self._A = np.load(f)\n",
    "\n",
    "        with open(\"save_agent_linucb_b_coldstart.bin\", 'rb') as f:\n",
    "            self._b = np.load(f)\n",
    "\n",
    "        self._invert(display=True)\n",
    "        print(\"linucb model loaded\")\n",
    "\n",
    "    def select(self, context, tie_break_mode = \"random\"):\n",
    "        value = np.zeros(self._nArms)\n",
    "        confidence = np.zeros(self._nArms)\n",
    "        for index,i_theta in enumerate(self._theta):\n",
    "\n",
    "            value[index] = np.dot(context,i_theta)[0]\n",
    "        for k in range(self._nArms):\n",
    "            confidence[k] = self._alpha * np.sqrt(np.dot(context,np.dot(self._A_inv[k],np.transpose(context))))\n",
    "        decision_space = [i for i,v in enumerate(np.squeeze(np.asarray(value + confidence)).ravel()) if v == np.max(np.squeeze(np.asarray(value + confidence)))]\n",
    "        if (tie_break_mode == \"min\"):\n",
    "            #if more than one possible action choose the first one\n",
    "            best_action = np.min(decision_space)\n",
    "        elif (tie_break_mode == \"max\"):\n",
    "            #if more than one possible action choose the last one\n",
    "            best_action = np.max(decision_space)\n",
    "        else:\n",
    "            #if more than one possible action choose randomly\n",
    "            best_action = np.random.choice(decision_space)\n",
    "\n",
    "        return int(best_action), value, confidence\n",
    "\n",
    "    def observe(self, played_arm, context, next_context, reward, update = False):\n",
    "        self._A[played_arm] = self._A[played_arm] + np.dot(np.transpose(context),context)\n",
    "        self._b[played_arm] = self._b[played_arm] + np.transpose(context * reward)\n",
    "        if update or ((self.t+1)%self._inversion_interval == 0):\n",
    "            self._invert(display=True)\n",
    "        self.t += 1\n",
    "        \n",
    "    def clean(self, played_arm, context, next_context, reward, update = False):\n",
    "        self._A[played_arm] = self._A[played_arm] - np.dot(np.transpose(context), context)\n",
    "        self._b[played_arm] = self._b[played_arm] - np.transpose(context * reward)\n",
    "        if update or ((self.t+1)%self._inversion_interval == 0):\n",
    "            self._invert(display=True)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self._A = [self._lambda * np.eye(self._nFeature) for k in range(self._nArms)]\n",
    "        self._b = [np.zeros((self._nFeature, 1)) for k in range(self._nArms)]\n",
    "        self._invert(display=True)\n",
    "\n",
    "    def _invert(self,display = False):\n",
    "        self._A_inv = [np.linalg.inv(a) for a in self._A]\n",
    "        self._theta = [np.dot(self._A_inv[k],self._b[k]) for k in range(self._nArms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit HSLinUCB in hotstart with already seen contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment setup\n",
    "mode='hotstart_unseen'\n",
    "debug = False\n",
    "latency_ref = 600\n",
    "starting_level = 1 #The environement will start at this specified level\n",
    "max_level = 10 #10 #The number of levels that can be managed in the environment\n",
    "#Agent setup\n",
    "nrounds = 9000\n",
    "Arms=2 #Number of actions\n",
    "exploration = 1\n",
    "_lambda = 1 #regularization parameter\n",
    "tie_break_mode = \"random\" #Action selection mode when tied. Possible value: \"random\", \"min\", \"max\"\n",
    "displayDynamicGraph = True #If True display regret and environment change dynamicaly. This dramatically increases the simulation time. Change this value require a full restart of the notebook as the graphic rendering engine change.\n",
    "ExperimentNumber = 10\n",
    "environment_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "optimal_environment_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "latency_exp = np.empty((ExperimentNumber, nrounds))\n",
    "latency_reference_exp = np.empty((ExperimentNumber, nrounds))\n",
    "injector_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "for experiment in trange(ExperimentNumber):\n",
    "    #Create environment\n",
    "    Environment = EnvTest(Arms,level=starting_level,max_level = max_level,latency_ref = latency_ref, debug = debug)\n",
    "    Environment.load_PatternModel()\n",
    "    Environment.computeOracle()\n",
    "    #Create Agent\n",
    "    Agent = LinUCB(Arms, nFeature = 7 ,alpha = exploration, _lambda = _lambda)\n",
    "    Agent.load_model() #Load coldstart Model\n",
    "    #Start simulation\n",
    "    injector_level,latency_ref_history,latency, environment_level, optimal_environment_level = Evaluate(Environment, Agent, nrounds=nrounds, tie_break_mode=tie_break_mode ,DisplayCumulativeRewardGraph = displayDynamicGraph, debug = debug)\n",
    "    injector_level_exp[experiment] = injector_level\n",
    "    latency_reference_exp[experiment] = latency_ref_history\n",
    "    latency_exp[experiment] = latency\n",
    "    environment_level_exp[experiment] = environment_level\n",
    "    optimal_environment_level_exp[experiment] = optimal_environment_level\n",
    "    \n",
    "with open(\"hslinucb_injector_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , injector_level_exp)\n",
    "with open(\"hslinucb_latency_reference_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , latency_reference_exp)\n",
    "with open(\"hslinucb_latency_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , latency_exp)\n",
    "with open(\"hslinucb_environment_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , environment_level_exp)\n",
    "with open(\"hslinucb_optimal_environment_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , optimal_environment_level_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
