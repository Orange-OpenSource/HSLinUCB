{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Software Name : HSLinUCB\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2021 Orange\n",
    "# SPDX-License-Identifier: GPL-2.0\n",
    "#\n",
    "# This software is distributed under the GNU General Public License v2.0 license\n",
    "#\n",
    "# Author: David DELANDE <david.delande@orange.com> et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This jupyter notebook requires a Deep Q-Learning model. Executes dqn_coldstart.ipynb to generate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from math import *\n",
    "import scipy.stats as stats\n",
    "from numpy.linalg import inv\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy.linalg\n",
    "import copy\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm import trange\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.random.seed(1)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as Kb\n",
    "tf.compat.v1.set_random_seed(1)\n",
    "import pickle\n",
    "from collections import deque , OrderedDict\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import h5py\n",
    "from lib.CogscalingLib import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionFile=\"D2.h5\" #Set the dataset to use. Other possible dataset: D2.h5. For ColdStart and hotstart seen uses D1.h5. For hotstart unseen uses D2.h5\n",
    "environment_orchestrator = Orchestrator(debug=False, sessionFile=sessionFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvTest():\n",
    "\n",
    "    \n",
    "    def __init__(self, narms, level=1,max_level = 25, latency_ref = 600, debug = False):\n",
    "        self.latency_ref = latency_ref\n",
    "        self.min_level = 1\n",
    "        self.max_level = max_level\n",
    "        self.level = level\n",
    "        self.debug = debug\n",
    "        self.narms = narms\n",
    "        self.loads = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "        self.levels = [1,2,3,4,5,6,7,8,9,10]\n",
    "        self.sessionFile=sessionFile\n",
    "        self.oracle_cheat = np.full((np.max(self.loads) +1, len(self.levels) + 1),2)\n",
    "        self.previous_states = []\n",
    "        self.current_states = []\n",
    "        self.previous_duration = 0\n",
    "        self.current_duration = 0\n",
    "        self.previous_rq = 0\n",
    "        self.current_rq = 0\n",
    "        self.targetPattern = []\n",
    "        self.rounds = 0\n",
    "        self.replay = True\n",
    "        self.record = False\n",
    "        super().__init__()\n",
    "        return\n",
    "    \n",
    "    def reset(self, narms, target, level,max_level, latency_ref):\n",
    "        self.level = level\n",
    "        self.rounds = 0\n",
    "        self.step = 0\n",
    "    \n",
    "    def moving_mean(self,measure,order):\n",
    "        result = []\n",
    "        if order%2 == 0:\n",
    "            m = order / 2\n",
    "            m = int(m)\n",
    "            for index in range(0, len(measure)):\n",
    "                if index < m or index + m + 1 > len(measure):\n",
    "                    continue\n",
    "                sum1 = 0\n",
    "                sum2 = 0\n",
    "                for index_value in range(index -m, index + m):\n",
    "                    sum1 = sum1 + measure[index_value]\n",
    "                    sum2 = sum2 + measure[index_value + 1]\n",
    "                mean1 = (1/(2*m)) * sum1\n",
    "                mean2 = (1/(2*m)) * sum2\n",
    "                result.append((mean1 + mean2) / 2)\n",
    "        else:\n",
    "            m = (order - 1) / 2\n",
    "            m = int(m)\n",
    "            for index in range(0, len(measure)):\n",
    "                if index < m or index + m + 1 > len(measure):\n",
    "                    continue   \n",
    "                sum1 = 0\n",
    "                for index_value in range(index - m, index + m + 1):\n",
    "                    sum1 = sum1 + measure[index_value]\n",
    "                result.append((1/((2*m)+1)) * sum1)\n",
    "        return result\n",
    "    \n",
    "    def save_PatternModel(self,file = ''):\n",
    "        #Save the target pattern to a json file\n",
    "        if file == '':\n",
    "            file = 'pattern'\n",
    "        with open(file + '.json', 'w') as filehandle:\n",
    "            json.dump(self.targetPattern, filehandle)\n",
    "\n",
    "    def load_PatternModel(self, file = ''):\n",
    "        #Load the target pattern from a json file\n",
    "        if file == '':\n",
    "            file = 'pattern'\n",
    "        with open(file + '.json') as json_file:\n",
    "            self.targetPattern = json.load(json_file)\n",
    "        return self.targetPattern\n",
    "    \n",
    "    def display_PatternModel(self):\n",
    "        #Generate the target pattern graph\n",
    "        f = plt.figure()\n",
    "        patternGraph = f.add_subplot(111)\n",
    "        patternGraph.plot(self.targetPattern)\n",
    "        plt.ylabel(\"Number of users\",fontsize=16)\n",
    "        plt.xlabel(\"Steps\",fontsize=16)\n",
    "        plt.title(\"Workload injection pattern\");\n",
    "        plt.grid();\n",
    "        plt.show()\n",
    "    \n",
    "    def generateRealProgressiveTargetPattern(self,nrounds,step,target_list):\n",
    "        #Generate the target pattern\n",
    "        self.targetPattern = []\n",
    "        patternChangeNumber = math.floor(nrounds/step)\n",
    "\n",
    "        target = target_list[0]\n",
    "        target_list_index = 0\n",
    "        mode = 1 #1 for increase, 0 for decrease\n",
    "        p_c = 1\n",
    "        for n in range(nrounds): \n",
    "                if n < (p_c * step):\n",
    "                    self.targetPattern.append(target)\n",
    "                elif p_c <= patternChangeNumber:\n",
    "                    p_c = p_c + 1\n",
    "                    if mode == 1:\n",
    "                        if target_list_index < (len(target_list) - 1):\n",
    "                            target_list_index += 1\n",
    "                        else:\n",
    "                            mode = 0\n",
    "                    else:\n",
    "                        if target_list_index > 0:\n",
    "                            target_list_index -= 1\n",
    "                        else:\n",
    "                            mode = 1\n",
    "                    target = target_list[target_list_index]\n",
    "                    self.targetPattern.append(target)\n",
    "                else:\n",
    "                    self.targetPattern.append(target)\n",
    "        return self.targetPattern\n",
    "    \n",
    "    def changeInjector(self):\n",
    "        user_level = self.targetPattern[self.rounds]\n",
    "        if self.debug:\n",
    "            print(\"injector level:\", user_level)\n",
    "        if self.replay == False:\n",
    "            status, message = environment_orchestrator.setLocustUser(user=int(user_level),spawn_rate=1)\n",
    "            print(\"message:\", message)\n",
    "                                    \n",
    "    def getContext(self):\n",
    "        states = []\n",
    "\n",
    "            \n",
    "        if len(self.previous_states) == 0:\n",
    "            state = environment_orchestrator.getAgregatedState(components=[{'prometheus': 'front-dynamic-component','zipkin': 'front-dynamic-component-service.default.svc.cluster.local:80/*'}],replay=self.replay, record=self.record,load=self.targetPattern[self.rounds], level=self.level,useMetricServer = False)\n",
    "            self.previous_states.append(state['lastcomponentNumber'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['duration'].to_numpy().astype(float)[0])\n",
    "            self.previous_duration = state['duration'].to_numpy().astype(float)[0]\n",
    "            self.previous_states.append(state['req_perc_sec'].to_numpy().astype(float)[0])\n",
    "            self.previous_rq = state['req_perc_sec'].to_numpy().astype(float)[0]\n",
    "            self.previous_states.append(state['cpu_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['cpu_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['mem_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "            self.previous_states.append(state['mem_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        elif len(self.previous_states) != 0 and len(self.current_states) != 0:\n",
    "            self.previous_states = self.current_states\n",
    "            self.previous_duration = self.current_duration\n",
    "            self.previous_rq = self.current_rq\n",
    "\n",
    "        self.current_states = []\n",
    "        state = environment_orchestrator.getAgregatedState(components=[{'prometheus': 'front-dynamic-component','zipkin': 'front-dynamic-component-service.default.svc.cluster.local:80/*'}],replay=self.replay, record=self.record,load=self.targetPattern[self.rounds], level=self.level,useMetricServer = False)\n",
    "\n",
    "\n",
    "        response_times = state['response_times'][0] \n",
    "        num_reqs_perc_sec = state['num_reqs_per_sec'][0]\n",
    "        num_fail_per_sec = state['num_fail_per_sec'][0]\n",
    "        self.current_states.append(state['lastcomponentNumber'].to_numpy().astype(float)[0] / self.max_level)\n",
    "        self.level = int(state['lastcomponentNumber'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['duration'].to_numpy().astype(float)[0] / 20000)\n",
    "        self.current_duration = state['duration'].to_numpy().astype(float)[0]\n",
    "        self.current_states.append(state['req_perc_sec'].to_numpy().astype(float)[0] / 100)\n",
    "        self.current_rq = state['req_perc_sec'].to_numpy().astype(float)[0]\n",
    "        self.current_states.append(state['cpu_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['cpu_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['mem_perc_request_mean'].to_numpy().astype(float)[0])\n",
    "        self.current_states.append(state['mem_perc_limit_mean'].to_numpy().astype(float)[0])\n",
    "        if self.debug:\n",
    "            print(\"previous states in getcontext:\", self.previous_states)\n",
    "            print(\"current states in getcontext:\", self.current_states)\n",
    "        global_states = np.concatenate((self.previous_states,self.current_states), axis=None)\n",
    "        states = self.current_states\n",
    "        states = np.asmatrix(states)\n",
    "        if self.debug:\n",
    "            print(\"state returned:\", states)\n",
    "            \n",
    "        return states\n",
    "    \n",
    "    def armStay(self,context):\n",
    "        if self.debug:\n",
    "            print(\"In action stay function\")\n",
    "            print(\"level in armStay:\", self.level)\n",
    "        previous_context = context\n",
    "        reward = self.computeReward(previous_context=previous_context,context = context, action = \"stay\",actionStep = 0)\n",
    "        return reward, context\n",
    "        \n",
    "    def armUp(self,context,actionStep = 1):\n",
    "        previous_context = context\n",
    "        if self.debug:\n",
    "            print(\"In action up function\")\n",
    "            print(\"actionStep:\", actionStep)\n",
    "            print(\"level in armUp:\", self.level)\n",
    "        if self.level + actionStep > self.max_level:\n",
    "            if self.debug:\n",
    "                print(\"upper limit reached\")\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.replay:\n",
    "                self.level += actionStep\n",
    "            else:\n",
    "                status, message = environment_orchestrator.incrementalKubernetesDeploymentScale(deployment=\"front-dynamic-component\",step=actionStep,waitKubernetes=True,waitPrometheus=True,useMetricServer = False)\n",
    "                print(\"message:\", message)\n",
    "            context = self.getContext()\n",
    "            reward = self.computeReward(previous_context = previous_context,context = context, action = \"up\",actionStep = actionStep)\n",
    "        return reward,context\n",
    "    \n",
    "    def armDown(self,context, actionStep = 1):\n",
    "        previous_context = context\n",
    "        if self.debug:\n",
    "            print(\"In action down function\")\n",
    "            print(\"actionStep:\", actionStep)\n",
    "            print(\"level in armDown:\", self.level)\n",
    "        if self.level + actionStep < self.min_level:\n",
    "            if self.debug:\n",
    "                print(\"lower limit reached\")\n",
    "            reward = 0\n",
    "        else:\n",
    "            if self.debug:\n",
    "                print(\"lower limit not reached\")\n",
    "            if self.replay:\n",
    "                self.level += actionStep\n",
    "            else:\n",
    "                status, message = environment_orchestrator.incrementalKubernetesDeploymentScale(deployment=\"front-dynamic-component\",step=actionStep,waitKubernetes=True,waitPrometheus=True,useMetricServer = False)\n",
    "                print(\"message:\", message)\n",
    "            context = self.getContext()\n",
    "            reward = self.computeReward(previous_context = previous_context,context = context, action = \"down\",actionStep = actionStep)\n",
    "        return reward,context\n",
    "    \n",
    "    def computeReward(self,previous_context,context, action,actionStep = 1):\n",
    "        if self.debug:\n",
    "            print(\"in compute reward function\")\n",
    "        if action == \"up\":    \n",
    "\n",
    "            if self.debug:\n",
    "                print(\"compute reward for action up\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.previous_duration > self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        if action == \"down\":\n",
    "            if self.debug:\n",
    "                print(\"compute reward for action down\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.current_duration <= self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "                    \n",
    "        if action == \"stay\":\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"Compute reward for action stay\")\n",
    "                print(\"previous latency for reward computation:\", self.previous_duration)\n",
    "                print(\"latency for reward computation:\", self.current_duration)\n",
    "\n",
    "            if self.current_duration <= self.latency_ref:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "        if self.debug:\n",
    "            print(\"returned reward:\", reward)\n",
    "        return reward\n",
    "        \n",
    "    def computeOracle(self):\n",
    "        with h5py.File(self.sessionFile, \"r\") as f:\n",
    "            for load in self.loads:\n",
    "                bestset = False\n",
    "                for level in self.levels:\n",
    "                    load_grp = f.get(str(load))\n",
    "                    component_grp = load_grp.get('front-dynamic-component')\n",
    "                    level_grp = component_grp.get(str(level))\n",
    "                    d = level_grp.get('measure')\n",
    "                    data = np.asarray(d)\n",
    "                    duration = data[:,3].astype(float)\n",
    "                     \n",
    "                    if np.mean(duration) <= self.latency_ref and np.max(duration) <= self.latency_ref:\n",
    "                        if not bestset:\n",
    "                            self.oracle_cheat[load,level] = 1\n",
    "                            bestset = True\n",
    "                        else:\n",
    "                            self.oracle_cheat[load,level] = 0\n",
    "            if self.debug:\n",
    "                print(\"oracle cheat:\", self.oracle_cheat)\n",
    "\n",
    "    def oracle(self):\n",
    "        #Return the Oracle best action\n",
    "        return self.oracle_cheat[int(self.targetPattern[self.rounds]),int(self.level)]\n",
    "    \n",
    "    def oracleOptimalLevel(self):\n",
    "        #Return the Oracle best action\n",
    "        return np.where(self.oracle_cheat[int(self.targetPattern[self.rounds])] == 1)[0][0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate progressive pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrounds = 9000 #2400 #Number of step pattern to generate. It must be the same as the one that will be used in simulation\n",
    "randomTargetStep = 50 #The pattern will change every randomTargetStep\n",
    "\n",
    "target_list = [5,10,15,20,25,30,35,40,45,50]\n",
    "#Generate the environment\n",
    "Environment = EnvTest(narms=3,level=1,max_level = 4)\n",
    "#Generate the pattern\n",
    "pattern = Environment.generateRealProgressiveTargetPattern(nrounds = nrounds,step = randomTargetStep, target_list = target_list)\n",
    "Environment.save_PatternModel()\n",
    "#Load pattern from a json file(pattern.json). This method has to be called each time an environment needs to use the target pattern\n",
    "Environment.load_PatternModel()\n",
    "#Display graphically the target pattern model\n",
    "Environment.display_PatternModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(Environment, Agent, nrounds=None, tie_break_mode = \"random\",DisplayCumulativeRewardGraph = False, debug = False):\n",
    "\n",
    "    environment_level = []\n",
    "    optimal_environment_level = []\n",
    "    latency_history = []\n",
    "    latency_ref_history = []\n",
    "    injector_level = []\n",
    "\n",
    "    T = 0\n",
    "\n",
    "    for i in range(nrounds):\n",
    "\n",
    "        Environment.rounds = i\n",
    "        Environment.changeInjector()\n",
    "        \n",
    "        \n",
    "        context = Environment.getContext()\n",
    "        optimal_environment_level.append(Environment.oracleOptimalLevel())\n",
    "        if Environment.current_duration > Environment.latency_ref:\n",
    "            reward,next_context = Environment.armUp(context = context,actionStep = 1)\n",
    "        else:\n",
    "            action, estimated_reward, confidence = Agent.select(context,tie_break_mode)\n",
    "            if action == 0 :\n",
    "                reward,next_context = Environment.armDown(context = context,actionStep = -1)\n",
    "\n",
    "            if action == 1: \n",
    "                reward,next_context = Environment.armStay(context = context)\n",
    "            Agent.observe(action, context,next_context, reward, update = True)\n",
    "\n",
    "        latency_history.append(Environment.current_duration)\n",
    "        injector_level.append(Environment.targetPattern[i])\n",
    "        latency_ref_history.append(Environment.latency_ref)\n",
    "        environment_level.append(Environment.level)\n",
    "\n",
    "        T +=1\n",
    "        if (DisplayCumulativeRewardGraph and T %1000 == 0):\n",
    "            %matplotlib inline\n",
    "            clear_output(True)\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Number of users\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(injector_level)\n",
    "            ax.grid()\n",
    "            ax.set_title('Simulation hotstart unseen contexts injection pattern')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Number of containers\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(environment_level,label='dqn')\n",
    "            ax.plot(optimal_environment_level,label='Oracle')\n",
    "            ax.grid()\n",
    "            ax.set_title('simulation hotstart unseen contexts environment level')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc = 'upper right',prop={'size':20})\n",
    "            plt.show()\n",
    "            fig, ax = plt.subplots(figsize=(6, 4), nrows=1, ncols=1)\n",
    "            plt.xlabel('steps',fontsize=16)\n",
    "            plt.ylabel(\"Latency(ms)\",fontsize=16)\n",
    "            plt.xticks(fontsize=13,fontweight='normal')\n",
    "            plt.yticks(fontsize=13,fontweight='normal')\n",
    "            ax.plot(latency_history,label='dqn')\n",
    "            ax.plot(latency_ref_history,label=r'$l^{*}$')\n",
    "            ax.grid()\n",
    "            ax.set_title('simulation hotstart unseen context latency')\n",
    "            plt.tight_layout()\n",
    "            plt.legend(loc = 'upper right',prop={'size':20})\n",
    "            plt.show()\n",
    "    \n",
    "    return injector_level,latency_ref_history,latency_history, environment_level, optimal_environment_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, K, D, alpha = 0.1, epsilon = 0.9, gamma = 0.9, epsilon_max = 1, epsilon_increment = 0.01,reward_mode = \"linear\", fill_memory = True, learn = True,replace_target_iter=100,batch_size=3000):\n",
    "        self._K = K\n",
    "        self._D = D\n",
    "        self.epsilon_start = epsilon\n",
    "        self.learn = learn\n",
    "        self.reward_mode = reward_mode\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "        \n",
    "        self.variables_list = ['s','s_','a','r']\n",
    "        self.n_actions = K\n",
    "        self.n_features = D\n",
    "        self.lr = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = epsilon_increment\n",
    "        #Memory section\n",
    "        self.batch_size = batch_size\n",
    "        self.counter = 0\n",
    "        self.mem = pd.DataFrame(dtype=np.float64)\n",
    "        self.batch_memory = OrderedDict()\n",
    "        \n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # reduce keras overhead\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'dqn_model_dump_coldstart.h5'\n",
    "        self.q_model = self.build_neural_net()\n",
    "        # target Q Network\n",
    "        self.target_q_model = self.build_neural_net()\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.replace_target_network()\n",
    "        print(\"Agent init done\")\n",
    "        \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def save_model(self):\n",
    "        #This function saves the neural network model (weights)\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "        print(\"Neural network saved\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        #This function restores a neural network model (weights)\n",
    "        print(\"loading model\")\n",
    "        self.q_model.load_weights(self.weights_file)\n",
    "        self.replace_target_network()\n",
    "        print(\"Neural network loaded\")\n",
    "    \n",
    "    def export_model(self):\n",
    "        #model_theta and q_table are only here for signature compatibility\n",
    "        model_theta = []\n",
    "        q_table = []\n",
    "        return q_table, model_theta\n",
    "\n",
    "    def build_neural_net(self):\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_shape=(self.n_features,), activation='relu', kernel_initializer=init))\n",
    "        model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "        model.add(keras.layers.Dense(self.n_actions, activation='linear', kernel_initializer=init))\n",
    "        model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=self.lr), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, clean=False):\n",
    "        state = np.ravel(s)\n",
    "        state = np.append(state,a)\n",
    "        state = np.append(state,r)\n",
    "        state = np.append(state,np.ravel(s_))\n",
    "        memory = pd.DataFrame([state], dtype=np.float64)\n",
    "        self.mem = self.mem.append(memory, ignore_index=True)\n",
    "        if (clean):\n",
    "            self.clean_memory()\n",
    "                  \n",
    "    def clean_memory(self):\n",
    "        #This function removes all duplicates transition from memory\n",
    "        self.mem.drop_duplicates(subset=None, inplace=True)\n",
    "    \n",
    "    def save_memory(self,json=False):\n",
    "        #This function saves the memory in a json format if json=True or in binary format\n",
    "        self.clean_memory()\n",
    "        if (json):\n",
    "            self.mem.to_json (r'./dqn_memory_dump_coldstart.json')\n",
    "            print(\"Memory saved in json\")\n",
    "        else:\n",
    "            with open('dqn_memory_dump_coldstart.bin', 'wb') as memory_save_dump:\n",
    "                pickle.dump(self.mem, memory_save_dump)\n",
    "            print(\"Memory saved in binary\")\n",
    "\n",
    "    def load_memory(self,json=False):\n",
    "        #This function loads a json file into memory if json=True or from a binary file\n",
    "        if (json):\n",
    "            self.mem = pd.read_json(r'./dqn_memory_dump_coldstart.json')\n",
    "            print(\"Memory loaded from json file\")\n",
    "        else:\n",
    "            with open('dqn_memory_dump_coldstart.bin', 'rb') as memory_load_dump:\n",
    "                self.mem = pickle.load(memory_load_dump)\n",
    "            print(\"Memory loaded from binary file\")\n",
    "\n",
    "    def dump_memory(self):\n",
    "        #This function display the memory content\n",
    "        print(\"MEMORY DUMP:\" + str(self.mem))\n",
    "            \n",
    "    def batch_sample(self):\n",
    "        #This function retrieves a batch of transition from memory\n",
    "        self.counter = len(self.mem)\n",
    "        if(self.counter < self.batch_size): # Less transitions in memory than requested batch size\n",
    "            sample_index = np.random.choice(np.arange(self.counter), size=self.counter,replace = False)\n",
    "        else: #Enough transitions in memory so retrieves a full batch size\n",
    "            sample_index = np.random.choice(np.arange(len(self.mem)), size=self.batch_size,replace = False)\n",
    "        random.shuffle(sample_index)\n",
    "        self.batch_memory['s'] = np.array(self.mem.iloc[sample_index, 0:(self.n_features)])\n",
    "        self.batch_memory['a'] = np.array(self.mem.iloc[sample_index, (self.n_features)])\n",
    "        self.batch_memory['r'] = np.array(self.mem.iloc[sample_index, (self.n_features) + 1])\n",
    "        self.batch_memory['s_'] = np.array(self.mem.iloc[sample_index, (self.n_features) + 2:])\n",
    "        return self.batch_memory\n",
    "    \n",
    "    def select(self, context, tie_break_mode = \"random\"):\n",
    "        observation = context\n",
    "        action = self.select_next_action(observation, tie_break_mode = tie_break_mode)\n",
    "        value = np.zeros(self._K) #Not used. Only here for global evaluation compatibility\n",
    "        confidence = np.zeros(self._K) #Not used. Only here for global evaluation compatibility\n",
    "        return int(action), value, confidence\n",
    "        \n",
    "    def select_next_action(self, context, tie_break_mode = \"random\"):\n",
    "        if (np.shape(context)[0] != 1):\n",
    "            context = context[np.newaxis, :]\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            q_values = self.q_model.predict(context)\n",
    "            action = np.argmax(q_values)\n",
    "        else: #random action\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return int(action)\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        #This function copy weights to target network weights.\n",
    "        print(\"target network updated!\")\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "    \n",
    "    def increment_epsilon(self):\n",
    "        #This function increments current epsilon with epsilon_increment value up to epsilon_max\n",
    "        if (self.epsilon < self.epsilon_max):\n",
    "            #Increment epsilon\n",
    "            self.epsilon += self.epsilon_increment\n",
    "        else:\n",
    "            #Force epsilon to epsilon_max\n",
    "            self.epsilon = self.epsilon_max\n",
    "    \n",
    "    #def learn(self):\n",
    "    def observe(self, played_arm, context, next_context,reward, update = False):\n",
    "        #This is the main learning function\n",
    "        if self.learn_step_counter % 500 == 0 and (fill_memory or learn):\n",
    "            self.clean_memory() #Remove duplicate entry in memory\n",
    "            print(\"memory cleaned\")\n",
    "            print(\"memory size:\", len(self.mem))\n",
    "        if fill_memory:\n",
    "            self.store_transition(context, played_arm, reward, next_context,clean=False) #Store transition in memory\n",
    "        # check to replace fixed target network weights\n",
    "        if learn and self.learn_step_counter % 5 and len(self.mem) >= self.batch_size:\n",
    "            if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "                self.replace_target_network()\n",
    "        \n",
    "            batch_transition = self.batch_sample() #Retrieves a batch from memory\n",
    "        \n",
    "            q_next = self.target_q_model.predict(batch_transition['s_'])\n",
    "            q_eval = self.q_model.predict(batch_transition['s'])\n",
    "            q_target = q_eval.copy()\n",
    "            batch_index = np.arange(len(batch_transition['s']), dtype=np.int32)\n",
    "            eval_act_index = batch_transition['a'].astype(int)\n",
    "            reward = batch_transition['r']\n",
    "            q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "            history = self.q_model.fit(batch_transition['s'],\n",
    "                             q_target,\n",
    "                             batch_size=len(batch_transition['s']),\n",
    "                             epochs=1,\n",
    "                             verbose=0)\n",
    "            self.loss_history.append(history.history['loss'][0])\n",
    "        self.learn_step_counter += 1\n",
    "        self.increment_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit deep qlearning in hotstart with unseen contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='hotstart_unseen'\n",
    "debug = False\n",
    "starting_level = 1 #The environement will start at this specified level\n",
    "max_level = 10 #The number of levels that can be managed in the environment\n",
    "latency_ref = 600\n",
    "#Agent setup\n",
    "nrounds = 9000\n",
    "learning_step = 0\n",
    "alpha = 0.01\n",
    "gamma = 0.9\n",
    "replace_target_iter=100\n",
    "batch_size=50\n",
    "Arms=2 #Number of actions (must be impair)\n",
    "#Exploration rate will vary between \"starting_exploration\" and \"stopping_exploration\". The exploration rate will be increase at each round by increase_exploration_step\n",
    "starting_exploration = 1 #Starting exploration rate.Must be between 0 and 1\n",
    "stopping_exploration = 1 #Max value for exploration rate. Must be between 0 and 1\n",
    "increase_exploration_step =   0 #value added to current exploration rate at each step. Set to 0 for fixed exploration rate.\n",
    "tie_break_mode = \"random\" #Action selection mode when tied. Possible value: \"random\", \"min\", \"max\"\n",
    "displayDynamicGraph = True #If True display regret and environment change dynamicaly. This dramatically increases the simulation time. Change this value require a full restart of the notebook as the graphic rendering engine change.\n",
    "fill_memory = True\n",
    "learn = True\n",
    "ExperimentNumber = 10\n",
    "environment_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "optimal_environment_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "latency_exp = np.empty((ExperimentNumber, nrounds))\n",
    "latency_reference_exp = np.empty((ExperimentNumber, nrounds))\n",
    "injector_level_exp = np.empty((ExperimentNumber, nrounds))\n",
    "for experiment in trange(ExperimentNumber):\n",
    "    #Create the environment\n",
    "    Environment = EnvTest(Arms,level=starting_level,max_level = max_level,latency_ref = latency_ref, debug = debug)\n",
    "    Environment.load_PatternModel()\n",
    "    Environment.computeOracle()\n",
    "    #Create agent\n",
    "    Agent = DeepQNetwork(Arms, D = 7, alpha = alpha, epsilon = starting_exploration, gamma = gamma, epsilon_max = stopping_exploration, epsilon_increment = increase_exploration_step,learn = learn, fill_memory = fill_memory,replace_target_iter=replace_target_iter,batch_size=batch_size)\n",
    "    Agent.load_model()\n",
    "    Agent.load_memory()\n",
    "    #Start simulation\n",
    "    injector_level,latency_ref_history,latency, environment_level, optimal_environment_level = Evaluate(Environment, Agent, nrounds=nrounds, tie_break_mode=tie_break_mode,DisplayCumulativeRewardGraph = displayDynamicGraph, debug = debug)\n",
    "    injector_level_exp[experiment] = injector_level\n",
    "    latency_reference_exp[experiment] = latency_ref_history\n",
    "    latency_exp[experiment] = latency\n",
    "    environment_level_exp[experiment] = environment_level\n",
    "    optimal_environment_level_exp[experiment] = optimal_environment_level\n",
    "with open(\"deepqlearning_injector_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , injector_level_exp)\n",
    "with open(\"deepqlearning_latency_reference_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , latency_reference_exp)\n",
    "with open(\"deepqlearning_latency_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , latency_exp)\n",
    "with open(\"deepqlearning_environment_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , environment_level_exp)\n",
    "with open(\"deepqlearning_optimal_environment_level_\" + mode + \".bin\", 'wb') as f:\n",
    "    np.save(f , optimal_environment_level_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
